{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafio Latam\n",
    "\n",
    "### 1. ¿Cómo se distribuyen los datos? ¿Qué te llama la atención o cuál es tu conclusión sobre esto?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comenzar el análisis de los datos, abrimos la base de datos \"Dataset_scl\" mediante la función read_csv de Pandas. Posterior a esto se pasa una función para describir la base de datos y conocer la distribución de sus variables. Se especifíca que se quiere utilizar todo tipo de datos para que no nos entregue como resultado solamente las columnas de tipo \"int\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matias U\\AppData\\Local\\Temp\\ipykernel_5256\\645654622.py:4: DtypeWarning: Columns (1,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataset_SCL = pd.read_csv(r'C:\\Users\\Matias U\\Documents\\flight-delays-cl\\dataset_SCL.csv')\n"
     ]
    }
   ],
   "source": [
    "# Imported C:\\Users\\Matias U\\Documents\\flight-delays-cl\\dataset_SCL.csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "dataset_SCL = pd.read_csv(r'C:\\Users\\Matias U\\Documents\\flight-delays-cl\\dataset_SCL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div id=8a4ec835-214d-4eb8-972f-191d7357322d style=\"display:none; background-color:#9D6CFF; color:white; width:200px; height:30px; padding-left:5px; border-radius:4px; flex-direction:row; justify-content:space-around; align-items:center;\" onmouseover=\"this.style.backgroundColor='#BA9BF8'\" onmouseout=\"this.style.backgroundColor='#9D6CFF'\" onclick=\"window.commands?.execute('create-mitosheet-from-dataframe-output');\">See Full Dataframe in Mito</div> <script> if (window.commands.hasCommand('create-mitosheet-from-dataframe-output')) document.getElementById('8a4ec835-214d-4eb8-972f-191d7357322d').style.display = 'flex' </script> <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fecha_I</th>\n",
       "      <th>Vlo-I</th>\n",
       "      <th>Ori-I</th>\n",
       "      <th>Des-I</th>\n",
       "      <th>Emp-I</th>\n",
       "      <th>Fecha_O</th>\n",
       "      <th>Vlo-O</th>\n",
       "      <th>Ori-O</th>\n",
       "      <th>Des-O</th>\n",
       "      <th>Emp-O</th>\n",
       "      <th>DIA</th>\n",
       "      <th>MES</th>\n",
       "      <th>AÑO</th>\n",
       "      <th>DIANOM</th>\n",
       "      <th>TIPOVUELO</th>\n",
       "      <th>OPERA</th>\n",
       "      <th>SIGLAORI</th>\n",
       "      <th>SIGLADES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>68206</td>\n",
       "      <td>68206</td>\n",
       "      <td>68206</td>\n",
       "      <td>68206</td>\n",
       "      <td>68206</td>\n",
       "      <td>68206</td>\n",
       "      <td>68205</td>\n",
       "      <td>68206</td>\n",
       "      <td>68206</td>\n",
       "      <td>68206</td>\n",
       "      <td>68206.00000</td>\n",
       "      <td>68206.000000</td>\n",
       "      <td>68206.000000</td>\n",
       "      <td>68206</td>\n",
       "      <td>68206</td>\n",
       "      <td>68206</td>\n",
       "      <td>68206</td>\n",
       "      <td>68206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>53252</td>\n",
       "      <td>750</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>62774</td>\n",
       "      <td>774</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>28-07-17 13:30</td>\n",
       "      <td>174</td>\n",
       "      <td>SCEL</td>\n",
       "      <td>SCFA</td>\n",
       "      <td>LAN</td>\n",
       "      <td>05-11-17 14:51</td>\n",
       "      <td>174</td>\n",
       "      <td>SCEL</td>\n",
       "      <td>SCFA</td>\n",
       "      <td>LAN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Viernes</td>\n",
       "      <td>N</td>\n",
       "      <td>Grupo LATAM</td>\n",
       "      <td>Santiago</td>\n",
       "      <td>Buenos Aires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>6</td>\n",
       "      <td>686</td>\n",
       "      <td>68206</td>\n",
       "      <td>5787</td>\n",
       "      <td>37611</td>\n",
       "      <td>5</td>\n",
       "      <td>686</td>\n",
       "      <td>68206</td>\n",
       "      <td>5786</td>\n",
       "      <td>20988</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10292</td>\n",
       "      <td>36966</td>\n",
       "      <td>40892</td>\n",
       "      <td>68206</td>\n",
       "      <td>6335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.71479</td>\n",
       "      <td>6.622585</td>\n",
       "      <td>2017.000029</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2017.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2017.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.00000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2017.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.00000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2017.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.00000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2018.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div>"
      ],
      "text/plain": [
       "               Fecha_I  Vlo-I  Ori-I  Des-I  Emp-I         Fecha_O  Vlo-O  \\\n",
       "count            68206  68206  68206  68206  68206           68206  68205   \n",
       "unique           53252    750      1     64     30           62774    774   \n",
       "top     28-07-17 13:30    174   SCEL   SCFA    LAN  05-11-17 14:51    174   \n",
       "freq                 6    686  68206   5787  37611               5    686   \n",
       "mean               NaN    NaN    NaN    NaN    NaN             NaN    NaN   \n",
       "std                NaN    NaN    NaN    NaN    NaN             NaN    NaN   \n",
       "min                NaN    NaN    NaN    NaN    NaN             NaN    NaN   \n",
       "25%                NaN    NaN    NaN    NaN    NaN             NaN    NaN   \n",
       "50%                NaN    NaN    NaN    NaN    NaN             NaN    NaN   \n",
       "75%                NaN    NaN    NaN    NaN    NaN             NaN    NaN   \n",
       "max                NaN    NaN    NaN    NaN    NaN             NaN    NaN   \n",
       "\n",
       "        Ori-O  Des-O  Emp-O           DIA           MES           AÑO  \\\n",
       "count   68206  68206  68206  68206.000000  68206.000000  68206.000000   \n",
       "unique      1     63     32           NaN           NaN           NaN   \n",
       "top      SCEL   SCFA    LAN           NaN           NaN           NaN   \n",
       "freq    68206   5786  20988           NaN           NaN           NaN   \n",
       "mean      NaN    NaN    NaN     15.714790      6.622585   2017.000029   \n",
       "std       NaN    NaN    NaN      8.782886      3.523321      0.005415   \n",
       "min       NaN    NaN    NaN      1.000000      1.000000   2017.000000   \n",
       "25%       NaN    NaN    NaN      8.000000      3.000000   2017.000000   \n",
       "50%       NaN    NaN    NaN     16.000000      7.000000   2017.000000   \n",
       "75%       NaN    NaN    NaN     23.000000     10.000000   2017.000000   \n",
       "max       NaN    NaN    NaN     31.000000     12.000000   2018.000000   \n",
       "\n",
       "         DIANOM TIPOVUELO        OPERA  SIGLAORI      SIGLADES  \n",
       "count     68206     68206        68206     68206         68206  \n",
       "unique        7         2           23         1            62  \n",
       "top     Viernes         N  Grupo LATAM  Santiago  Buenos Aires  \n",
       "freq      10292     36966        40892     68206          6335  \n",
       "mean        NaN       NaN          NaN       NaN           NaN  \n",
       "std         NaN       NaN          NaN       NaN           NaN  \n",
       "min         NaN       NaN          NaN       NaN           NaN  \n",
       "25%         NaN       NaN          NaN       NaN           NaN  \n",
       "50%         NaN       NaN          NaN       NaN           NaN  \n",
       "75%         NaN       NaN          NaN       NaN           NaN  \n",
       "max         NaN       NaN          NaN       NaN           NaN  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_SCL.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ser en su mayoría datos de tipo \"string\" y fechas, no se puede obtener una tabla descriptiva con todos los datos que otorga la función describe, pero sí nos permite describir la frecuencia y el tipo de observación que más se repite dentro de las variables, otorgandonos información importante respecto a, por ejemplo, el tipo de vuelo más común dentro de los casos observados y le empresa a cargo de los vuelos.\n",
    "\n",
    "A modo general, algunas consideraciones importantes a tener en cuenta es que el tipo de vuelo con mayor prevalecencia dentro de nuestros datos es de tipo nacional, y que la empresa que opera la gran mayoría de los vuelos de nuestra data es el Grupo LATAM. \n",
    "\n",
    "A modo de hipótesis se puede plantear que el tipo de vuelo puede tener diferencias de escala que requieren distintos tipos de organización y coordinación entre aeropuertos, por lo que los vuelos internacionales estarían más propensos a retrasos. Posteriormente esto se verifica al haber una mayor cantidad de retrasos en vuelos internacionales que nacionales, como es posible ver en el dataframe data_atraso_tipo_vuelo.\n",
    "\n",
    "También es importante la distribución de las aerolíneas a cargo de los vuelos ya que existen factores institucionales y estructurales que determinan los protocólos de cómo se manejan los retrasos y las preparaciones de los vuelos. También, al haber una prevalencia notoria de una aerolínea sobre el resto, debemos guiarnos por la tasa de retraso por aerolínea y no por los valores totales de atrasos por aerolínea, ya que nos entregaría información sesgada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Genera las columnas adicionales y luego expórtelas en un archivo synthetic_features.csv :\n",
    "### - temporada_alta : 1 si Fecha-I está entre 15-Dic y 3-Mar, o 15-Jul y 31-Jul, o 11-Sep y 30-Sep, 0 si no. \n",
    "### - dif_min : diferencia en minutos entre Fecha-O y Fecha-I . \n",
    "### - atraso_15 : 1 si dif_min > 15, 0 si no. \n",
    "### - periodo_dia : mañana (entre 5:00 y 11:59), tarde (entre 12:00 y 18:59) y noche (entre 19:00 y 4:59), en base a Fecha-I . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partimos la segunda parte del desafío llamando el paquete \"Mito\" que nos facilita el trabajo con bases de datos y el proceso de data wrangling mediante el uso de spreadsheets dentro de Python. Partimos generando la columna dif_min, que indica la diferencia en minutos entre las variables Fecha-O y Fecha-I. El código asociado a las operaciones efectuadas con Mito sobre la base de datos para esta primera tarea aparece a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matias U\\AppData\\Local\\Temp\\ipykernel_5256\\3706536298.py:4: DtypeWarning: Columns (1,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataset_SCL = pd.read_csv(r'C:\\Users\\Matias U\\Documents\\flight-delays-cl\\dataset_SCL.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Fecha_I  Vlo-I  Ori-I  Des-I  Emp-I         Fecha_O  Vlo-O  \\\n",
      "count            68206  68206  68206  68206  68206           68206  68205   \n",
      "unique           53252    750      1     64     30           62774    774   \n",
      "top     28-07-17 13:30    174   SCEL   SCFA    LAN  05-11-17 14:51    174   \n",
      "freq                 6    686  68206   5787  37611               5    686   \n",
      "mean               NaN    NaN    NaN    NaN    NaN             NaN    NaN   \n",
      "std                NaN    NaN    NaN    NaN    NaN             NaN    NaN   \n",
      "min                NaN    NaN    NaN    NaN    NaN             NaN    NaN   \n",
      "25%                NaN    NaN    NaN    NaN    NaN             NaN    NaN   \n",
      "50%                NaN    NaN    NaN    NaN    NaN             NaN    NaN   \n",
      "75%                NaN    NaN    NaN    NaN    NaN             NaN    NaN   \n",
      "max                NaN    NaN    NaN    NaN    NaN             NaN    NaN   \n",
      "\n",
      "        Ori-O  Des-O  Emp-O           DIA           MES           AÑO  \\\n",
      "count   68206  68206  68206  68206.000000  68206.000000  68206.000000   \n",
      "unique      1     63     32           NaN           NaN           NaN   \n",
      "top      SCEL   SCFA    LAN           NaN           NaN           NaN   \n",
      "freq    68206   5786  20988           NaN           NaN           NaN   \n",
      "mean      NaN    NaN    NaN     15.714790      6.622585   2017.000029   \n",
      "std       NaN    NaN    NaN      8.782886      3.523321      0.005415   \n",
      "min       NaN    NaN    NaN      1.000000      1.000000   2017.000000   \n",
      "25%       NaN    NaN    NaN      8.000000      3.000000   2017.000000   \n",
      "50%       NaN    NaN    NaN     16.000000      7.000000   2017.000000   \n",
      "75%       NaN    NaN    NaN     23.000000     10.000000   2017.000000   \n",
      "max       NaN    NaN    NaN     31.000000     12.000000   2018.000000   \n",
      "\n",
      "         DIANOM TIPOVUELO        OPERA  SIGLAORI      SIGLADES  \n",
      "count     68206     68206        68206     68206         68206  \n",
      "unique        7         2           23         1            62  \n",
      "top     Viernes         N  Grupo LATAM  Santiago  Buenos Aires  \n",
      "freq      10292     36966        40892     68206          6335  \n",
      "mean        NaN       NaN          NaN       NaN           NaN  \n",
      "std         NaN       NaN          NaN       NaN           NaN  \n",
      "min         NaN       NaN          NaN       NaN           NaN  \n",
      "25%         NaN       NaN          NaN       NaN           NaN  \n",
      "50%         NaN       NaN          NaN       NaN           NaN  \n",
      "75%         NaN       NaN          NaN       NaN           NaN  \n",
      "max         NaN       NaN          NaN       NaN           NaN  \n"
     ]
    }
   ],
   "source": [
    "# Imported C:\\Users\\Matias U\\Documents\\flight-delays-cl\\dataset_SCL.csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "dataset_SCL = pd.read_csv(r'C:\\Users\\Matias U\\Documents\\flight-delays-cl\\dataset_SCL.csv')\n",
    "\n",
    "print(dataset_SCL.describe(include='all'))\n",
    "\n",
    "# Added column new-column-drl2 to dataset_SCL\n",
    "dataset_SCL.insert(18, 'new-column-drl2', 0)\n",
    "\n",
    "# Changed Fecha_I from object to datetime\n",
    "dataset_SCL['Fecha_I'] = pd.to_datetime(dataset_SCL['Fecha_I'], infer_datetime_format=True, errors='coerce')\n",
    "\n",
    "# Changed Fecha_I from datetime64[ns] to datetime\n",
    "dataset_SCL['Fecha_I'] = dataset_SCL['Fecha_I']\n",
    "\n",
    "# Changed Fecha_O from object to datetime\n",
    "dataset_SCL['Fecha_O'] = pd.to_datetime(dataset_SCL['Fecha_O'], infer_datetime_format=True, errors='coerce')\n",
    "\n",
    "# Changed Fecha_O from datetime64[ns] to datetime\n",
    "dataset_SCL['Fecha_O'] = dataset_SCL['Fecha_O']\n",
    "\n",
    "# Renamed new-column-drl2 to Diferencia in dataset_SCL\n",
    "dataset_SCL.rename(columns={'new-column-drl2': 'dif_min'}, inplace=True)\n",
    "\n",
    "# Set new-column-drl2 in dataset_SCL to =Fecha_O-Fecha_I\n",
    "dataset_SCL['dif_min'] = dataset_SCL['Fecha_O']-dataset_SCL['Fecha_I']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez creada la nueva columna pasamos a generar las nuevas variables \"temporada_alta\", \"atraso_15\" y \"periodo_dia\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Fecha_I Vlo-I Ori-I Des-I Emp-I             Fecha_O Vlo-O  \\\n",
      "0     2017-01-01 23:30:00   226  SCEL  KMIA   AAL 2017-01-01 23:33:00   226   \n",
      "1     2017-02-01 23:30:00   226  SCEL  KMIA   AAL 2017-02-01 23:39:00   226   \n",
      "2     2017-03-01 23:30:00   226  SCEL  KMIA   AAL 2017-03-01 23:39:00   226   \n",
      "3     2017-04-01 23:30:00   226  SCEL  KMIA   AAL 2017-04-01 23:33:00   226   \n",
      "4     2017-05-01 23:30:00   226  SCEL  KMIA   AAL 2017-05-01 23:28:00   226   \n",
      "...                   ...   ...   ...   ...   ...                 ...   ...   \n",
      "68201 2017-12-22 14:55:00   400  SCEL  SPJC   JAT 2017-12-22 15:41:00   400   \n",
      "68202 2017-12-25 14:55:00   400  SCEL  SPJC   JAT 2017-12-25 15:11:00   400   \n",
      "68203 2017-12-27 14:55:00   400  SCEL  SPJC   JAT 2017-12-27 15:35:00   400   \n",
      "68204 2017-12-29 14:55:00   400  SCEL  SPJC   JAT 2017-12-29 15:08:00   400   \n",
      "68205 2017-12-31 14:55:00   400  SCEL  SPJC   JAT 2017-12-31 15:04:00   400   \n",
      "\n",
      "      Ori-O Des-O Emp-O  ...   AÑO     DIANOM  TIPOVUELO              OPERA  \\\n",
      "0      SCEL  KMIA   AAL  ...  2017    Domingo          I  American Airlines   \n",
      "1      SCEL  KMIA   AAL  ...  2017      Lunes          I  American Airlines   \n",
      "2      SCEL  KMIA   AAL  ...  2017     Martes          I  American Airlines   \n",
      "3      SCEL  KMIA   AAL  ...  2017  Miercoles          I  American Airlines   \n",
      "4      SCEL  KMIA   AAL  ...  2017     Jueves          I  American Airlines   \n",
      "...     ...   ...   ...  ...   ...        ...        ...                ...   \n",
      "68201  SCEL  SPJC   JAT  ...  2017    Viernes          I       JetSmart SPA   \n",
      "68202  SCEL  SPJC   JAT  ...  2017      Lunes          I       JetSmart SPA   \n",
      "68203  SCEL  SPJC   JAT  ...  2017  Miercoles          I       JetSmart SPA   \n",
      "68204  SCEL  SPJC   JAT  ...  2017    Viernes          I       JetSmart SPA   \n",
      "68205  SCEL  SPJC   JAT  ...  2017    Domingo          I       JetSmart SPA   \n",
      "\n",
      "       SIGLAORI SIGLADES dif_min atraso_15  temporada_alta    periodo_dia  \n",
      "0      Santiago    Miami       3         0               1   19:00 a 4:59  \n",
      "1      Santiago    Miami       9         0               1   19:00 a 4:59  \n",
      "2      Santiago    Miami       9         0               1   19:00 a 4:59  \n",
      "3      Santiago    Miami       3         0               0   19:00 a 4:59  \n",
      "4      Santiago    Miami      -2         0               0   19:00 a 4:59  \n",
      "...         ...      ...     ...       ...             ...            ...  \n",
      "68201  Santiago     Lima      46         1               1  12:00 a 18:59  \n",
      "68202  Santiago     Lima      16         1               1  12:00 a 18:59  \n",
      "68203  Santiago     Lima      40         1               1  12:00 a 18:59  \n",
      "68204  Santiago     Lima      13         0               1  12:00 a 18:59  \n",
      "68205  Santiago     Lima       9         0               1  12:00 a 18:59  \n",
      "\n",
      "[68206 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "#pregunta 2\n",
    "\n",
    "import datetime\n",
    "\n",
    "#dif_min en minutos\n",
    "\n",
    "dataset_SCL['dif_min'] = dataset_SCL['dif_min'].dt.total_seconds().div(60).astype(int)\n",
    "\n",
    "# atraso_15\n",
    "\n",
    "dataset_SCL[\"atraso_15\"] = np.where(dataset_SCL['dif_min'] > 15, \"1\",\"0\")\n",
    "\n",
    "# temporada_alta\n",
    "\n",
    "months = pd.to_datetime(dataset_SCL['Fecha_I'],unit='ms')\n",
    " \n",
    "doy = months.dt.dayofyear\n",
    "\n",
    "dataset_SCL[\"temporada_alta\"] = pd.cut(doy, bins=(0,62,196,212,254,273,349,366),labels=(\"1\",\"0\",\"1\",\"0\",\"1\",\"0\",\"1\"),ordered=False) \n",
    "\n",
    "# periodo_dia \n",
    "\n",
    "hours = pd.to_datetime(dataset_SCL['Fecha_I'], format='%H:%M:%S').dt.hour\n",
    "\n",
    "dataset_SCL['periodo_dia'] = pd.cut(hours,bins=[0,5,12,19,24],include_lowest=True, labels=['19:00 a 4:59','5:00 a 11:59','12:00 a 18:59','19:00 a 4:59'],ordered=False)\n",
    "\n",
    "# dataset con todas las variables\n",
    "\n",
    "print(dataset_SCL)                                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset de variables sinteticas\n",
    "\n",
    "synthetic_values = dataset_SCL[[\"dif_min\",\"atraso_15\",\"temporada_alta\",\"periodo_dia\"]]\n",
    "\n",
    "synthetic_values.to_csv(\"synthetic_features.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. ¿Cómo se compone la tasa de atraso por destino, aerolínea, mes del año, día de la semana, temporada, tipo de vuelo? ¿Qué variables esperarías que más influyeran en predecir atrasos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente utilizamos la función sheet de mitosheet para crear pivotes en los cuales cruzaremos la variable dependiente atraso_15 con el resto de las variables de interés correspondientes a la pregunta 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c8222ec8394222a194ece537eb4f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MitoWidget(analysis_data_json='{\"analysisName\": \"UUID-0a019cb4-1108-4f37-9197-b218dd217e9a\", \"code\": {\"imports…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mitosheet\n",
    "mitosheet.sheet(dataset_SCL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mitosheet import *; register_analysis('UUID-60e77fb6-00ab-4b60-8e26-2fe2b1d72868')\n",
    "    \n",
    "# Pivoted dataset_SCL into df2\n",
    "unused_columns = dataset_SCL.columns.difference(set(['atraso_15']).union(set(['OPERA'])).union(set({'atraso_15'})))\n",
    "tmp_df = dataset_SCL.drop(unused_columns, axis=1)\n",
    "pivot_table = tmp_df.pivot_table(\n",
    "    index=['atraso_15'],\n",
    "    columns=['OPERA'],\n",
    "    values=['atraso_15'],\n",
    "    aggfunc={'atraso_15': ['count']}\n",
    ")\n",
    "pivot_table.set_axis([flatten_column_header(col) for col in pivot_table.keys()], axis=1, inplace=True)\n",
    "dataset_SCL_pivot = pivot_table.reset_index()\n",
    "\n",
    "# Pivoted dataset_SCL into df3\n",
    "unused_columns = dataset_SCL.columns.difference(set(['OPERA']).union(set(['atraso_15'])).union(set({'atraso_15'})))\n",
    "tmp_df = dataset_SCL.drop(unused_columns, axis=1)\n",
    "pivot_table = tmp_df.pivot_table(\n",
    "    index=['OPERA'],\n",
    "    columns=['atraso_15'],\n",
    "    values=['atraso_15'],\n",
    "    aggfunc={'atraso_15': ['count']}\n",
    ")\n",
    "pivot_table.set_axis([flatten_column_header(col) for col in pivot_table.keys()], axis=1, inplace=True)\n",
    "dataset_SCL_pivot_1 = pivot_table.reset_index()\n",
    "\n",
    "# Deleted dataframe dataset_SCL_pivot\n",
    "del dataset_SCL_pivot\n",
    "\n",
    "# Added column new-column-s4yx to dataset_SCL_pivot_1\n",
    "dataset_SCL_pivot_1.insert(1, 'new-column-s4yx', 0)\n",
    "\n",
    "# Reordered atraso_15 count 0 in dataset_SCL_pivot_1\n",
    "dataset_SCL_pivot_1_columns = [col for col in dataset_SCL_pivot_1.columns if col != 'atraso_15 count 0']\n",
    "dataset_SCL_pivot_1_columns.insert(1, 'atraso_15 count 0')\n",
    "dataset_SCL_pivot_1 = dataset_SCL_pivot_1[dataset_SCL_pivot_1_columns]\n",
    "\n",
    "# Reordered atraso_15 count 1 in dataset_SCL_pivot_1\n",
    "dataset_SCL_pivot_1_columns = [col for col in dataset_SCL_pivot_1.columns if col != 'atraso_15 count 1']\n",
    "dataset_SCL_pivot_1_columns.insert(2, 'atraso_15 count 1')\n",
    "dataset_SCL_pivot_1 = dataset_SCL_pivot_1[dataset_SCL_pivot_1_columns]\n",
    "\n",
    "# Renamed new-column-s4yx to Tasa de atraso in dataset_SCL_pivot_1\n",
    "dataset_SCL_pivot_1.rename(columns={'new-column-s4yx': 'Tasa de atraso'}, inplace=True)\n",
    "\n",
    "# Set new-column-s4yx in dataset_SCL_pivot_1 to =(atraso_15 count 1/(atraso_15 count 0+atraso_15 count 1))*100\n",
    "dataset_SCL_pivot_1['Tasa de atraso'] = (dataset_SCL_pivot_1['atraso_15 count 1']/(dataset_SCL_pivot_1['atraso_15 count 0']+dataset_SCL_pivot_1['atraso_15 count 1']))*100\n",
    "\n",
    "# Set new-column-s4yx in dataset_SCL_pivot_1 to =(atraso_15 count 1/(atraso_15 count 0+atraso_15 count 1))\n",
    "dataset_SCL_pivot_1['Tasa de atraso'] = (dataset_SCL_pivot_1['atraso_15 count 1']/(dataset_SCL_pivot_1['atraso_15 count 0']+dataset_SCL_pivot_1['atraso_15 count 1']))\n",
    "\n",
    "# Pivoted dataset_SCL into df3\n",
    "unused_columns = dataset_SCL.columns.difference(set(['Des-O']).union(set(['atraso_15'])).union(set({'atraso_15'})))\n",
    "tmp_df = dataset_SCL.drop(unused_columns, axis=1)\n",
    "pivot_table = tmp_df.pivot_table(\n",
    "    index=['Des-O'],\n",
    "    columns=['atraso_15'],\n",
    "    values=['atraso_15'],\n",
    "    aggfunc={'atraso_15': ['count']}\n",
    ")\n",
    "pivot_table.set_axis([flatten_column_header(col) for col in pivot_table.keys()], axis=1, inplace=True)\n",
    "dataset_SCL_pivot = pivot_table.reset_index()\n",
    "\n",
    "# Set column atraso_15 count 0 at index 2 in dataset_SCL_pivot to NaN\n",
    "dataset_SCL_pivot.at[2, 'atraso_15 count 0'] = None\n",
    "\n",
    "# Set column atraso_15 count 0 at index 2 in dataset_SCL_pivot to NaN\n",
    "dataset_SCL_pivot.at[2, 'atraso_15 count 0'] = None\n",
    "\n",
    "# Set column atraso_15 count 0 at index 2 in dataset_SCL_pivot to 0\n",
    "dataset_SCL_pivot.at[2, 'atraso_15 count 0'] = 0\n",
    "\n",
    "# Set column atraso_15 count 1 at index 5 in dataset_SCL_pivot to 0\n",
    "dataset_SCL_pivot.at[5, 'atraso_15 count 1'] = 0\n",
    "\n",
    "# Set column atraso_15 count 1 at index 29 in dataset_SCL_pivot to 0\n",
    "dataset_SCL_pivot.at[29, 'atraso_15 count 1'] = 0\n",
    "\n",
    "# Set column atraso_15 count 1 at index 30 in dataset_SCL_pivot to 0\n",
    "dataset_SCL_pivot.at[30, 'atraso_15 count 1'] = 0\n",
    "\n",
    "# Set column atraso_15 count 0 at index 51 in dataset_SCL_pivot to 0\n",
    "dataset_SCL_pivot.at[51, 'atraso_15 count 0'] = 0\n",
    "\n",
    "# Set column atraso_15 count 0 at index 54 in dataset_SCL_pivot to 0\n",
    "dataset_SCL_pivot.at[54, 'atraso_15 count 0'] = 0\n",
    "\n",
    "# Set column atraso_15 count 1 at index 58 in dataset_SCL_pivot to 0\n",
    "dataset_SCL_pivot.at[58, 'atraso_15 count 1'] = 0\n",
    "\n",
    "# Changed atraso_15 count 0 from float64 to int\n",
    "dataset_SCL_pivot['atraso_15 count 0'] = dataset_SCL_pivot['atraso_15 count 0'].astype('int')\n",
    "\n",
    "# Changed atraso_15 count 1 from float64 to int\n",
    "dataset_SCL_pivot['atraso_15 count 1'] = dataset_SCL_pivot['atraso_15 count 1'].astype('int')\n",
    "\n",
    "# Added column new-column-ogtu to dataset_SCL_pivot\n",
    "dataset_SCL_pivot.insert(3, 'new-column-ogtu', 0)\n",
    "\n",
    "# Renamed new-column-ogtu to Atraso por destino in dataset_SCL_pivot\n",
    "dataset_SCL_pivot.rename(columns={'new-column-ogtu': 'Atraso por destino'}, inplace=True)\n",
    "\n",
    "# Renamed new-column-s4yx to Atraso por aerolinea in dataset_SCL_pivot_1\n",
    "dataset_SCL_pivot_1.rename(columns={'Tasa de atraso': 'Atraso por aerolinea'}, inplace=True)\n",
    "\n",
    "# Set new-column-ogtu in dataset_SCL_pivot to =atraso_15 count 1/(atraso_15 count 0+atraso_15 count 1)\n",
    "dataset_SCL_pivot['Atraso por destino'] = dataset_SCL_pivot['atraso_15 count 1']/(dataset_SCL_pivot['atraso_15 count 0']+dataset_SCL_pivot['atraso_15 count 1'])\n",
    "\n",
    "# Pivoted dataset_SCL into df4\n",
    "unused_columns = dataset_SCL.columns.difference(set(['MES']).union(set(['atraso_15'])).union(set({'atraso_15'})))\n",
    "tmp_df = dataset_SCL.drop(unused_columns, axis=1)\n",
    "pivot_table = tmp_df.pivot_table(\n",
    "    index=['MES'],\n",
    "    columns=['atraso_15'],\n",
    "    values=['atraso_15'],\n",
    "    aggfunc={'atraso_15': ['count']}\n",
    ")\n",
    "pivot_table.set_axis([flatten_column_header(col) for col in pivot_table.keys()], axis=1, inplace=True)\n",
    "dataset_SCL_pivot_2 = pivot_table.reset_index()\n",
    "\n",
    "# Added column new-column-0op3 to dataset_SCL_pivot_2\n",
    "dataset_SCL_pivot_2.insert(1, 'new-column-0op3', 0)\n",
    "\n",
    "# Reordered atraso_15 count 0 in dataset_SCL_pivot_2\n",
    "dataset_SCL_pivot_2_columns = [col for col in dataset_SCL_pivot_2.columns if col != 'atraso_15 count 0']\n",
    "dataset_SCL_pivot_2_columns.insert(1, 'atraso_15 count 0')\n",
    "dataset_SCL_pivot_2 = dataset_SCL_pivot_2[dataset_SCL_pivot_2_columns]\n",
    "\n",
    "# Reordered atraso_15 count 1 in dataset_SCL_pivot_2\n",
    "dataset_SCL_pivot_2_columns = [col for col in dataset_SCL_pivot_2.columns if col != 'atraso_15 count 1']\n",
    "dataset_SCL_pivot_2_columns.insert(2, 'atraso_15 count 1')\n",
    "dataset_SCL_pivot_2 = dataset_SCL_pivot_2[dataset_SCL_pivot_2_columns]\n",
    "\n",
    "# Renamed new-column-0op3 to Atraso por mes in dataset_SCL_pivot_2\n",
    "dataset_SCL_pivot_2.rename(columns={'new-column-0op3': 'Atraso por mes'}, inplace=True)\n",
    "\n",
    "# Set new-column-0op3 in dataset_SCL_pivot_2 to =atraso_15 count 1/(atraso_15 count 0+atraso_15 count 1)\n",
    "dataset_SCL_pivot_2['Atraso por mes'] = dataset_SCL_pivot_2['atraso_15 count 1']/(dataset_SCL_pivot_2['atraso_15 count 0']+dataset_SCL_pivot_2['atraso_15 count 1'])\n",
    "\n",
    "# Pivoted dataset_SCL into df5\n",
    "dataset_SCL_pivot_3 = pd.DataFrame(data={})\n",
    "\n",
    "# Deleted dataframe dataset_SCL_pivot_3\n",
    "del dataset_SCL_pivot_3\n",
    "\n",
    "# Pivoted dataset_SCL into df5\n",
    "unused_columns = dataset_SCL.columns.difference(set(['DIANOM']).union(set(['atraso_15'])).union(set({'atraso_15'})))\n",
    "tmp_df = dataset_SCL.drop(unused_columns, axis=1)\n",
    "pivot_table = tmp_df.pivot_table(\n",
    "    index=['DIANOM'],\n",
    "    columns=['atraso_15'],\n",
    "    values=['atraso_15'],\n",
    "    aggfunc={'atraso_15': ['count']}\n",
    ")\n",
    "pivot_table.set_axis([flatten_column_header(col) for col in pivot_table.keys()], axis=1, inplace=True)\n",
    "dataset_SCL_pivot_3 = pivot_table.reset_index()\n",
    "\n",
    "# Added column new-column-g5eu to dataset_SCL_pivot_3\n",
    "dataset_SCL_pivot_3.insert(3, 'new-column-g5eu', 0)\n",
    "\n",
    "# Set new-column-g5eu in dataset_SCL_pivot_3 to =atraso_15 count 1/(atraso_15 count 0+atraso_15 count 1)\n",
    "dataset_SCL_pivot_3['new-column-g5eu'] = dataset_SCL_pivot_3['atraso_15 count 1']/(dataset_SCL_pivot_3['atraso_15 count 0']+dataset_SCL_pivot_3['atraso_15 count 1'])\n",
    "\n",
    "# Pivoted dataset_SCL into df6\n",
    "unused_columns = dataset_SCL.columns.difference(set(['temporada_alta']).union(set(['atraso_15'])).union(set({'atraso_15'})))\n",
    "tmp_df = dataset_SCL.drop(unused_columns, axis=1)\n",
    "pivot_table = tmp_df.pivot_table(\n",
    "    index=['temporada_alta'],\n",
    "    columns=['atraso_15'],\n",
    "    values=['atraso_15'],\n",
    "    aggfunc={'atraso_15': ['count']}\n",
    ")\n",
    "pivot_table.set_axis([flatten_column_header(col) for col in pivot_table.keys()], axis=1, inplace=True)\n",
    "dataset_SCL_pivot_4 = pivot_table.reset_index()\n",
    "\n",
    "# Added column new-column-irc4 to dataset_SCL_pivot_4\n",
    "dataset_SCL_pivot_4.insert(3, 'new-column-irc4', 0)\n",
    "\n",
    "# Renamed new-column-irc4 to Atraso por temporada in dataset_SCL_pivot_4\n",
    "dataset_SCL_pivot_4.rename(columns={'new-column-irc4': 'Atraso por temporada'}, inplace=True)\n",
    "\n",
    "# Set new-column-irc4 in dataset_SCL_pivot_4 to =atraso_15 count 1/atraso_15 count 0+atraso_15 count 1\n",
    "dataset_SCL_pivot_4['Atraso por temporada'] = dataset_SCL_pivot_4['atraso_15 count 1']/dataset_SCL_pivot_4['atraso_15 count 0']+dataset_SCL_pivot_4['atraso_15 count 1']\n",
    "\n",
    "# Set new-column-irc4 in dataset_SCL_pivot_4 to =atraso_15 count 1/(atraso_15 count 0+atraso_15 count 1)\n",
    "dataset_SCL_pivot_4['Atraso por temporada'] = dataset_SCL_pivot_4['atraso_15 count 1']/(dataset_SCL_pivot_4['atraso_15 count 0']+dataset_SCL_pivot_4['atraso_15 count 1'])\n",
    "\n",
    "# Pivoted dataset_SCL into df7\n",
    "unused_columns = dataset_SCL.columns.difference(set(['TIPOVUELO']).union(set(['atraso_15'])).union(set({'atraso_15'})))\n",
    "tmp_df = dataset_SCL.drop(unused_columns, axis=1)\n",
    "pivot_table = tmp_df.pivot_table(\n",
    "    index=['TIPOVUELO'],\n",
    "    columns=['atraso_15'],\n",
    "    values=['atraso_15'],\n",
    "    aggfunc={'atraso_15': ['count']}\n",
    ")\n",
    "pivot_table.set_axis([flatten_column_header(col) for col in pivot_table.keys()], axis=1, inplace=True)\n",
    "dataset_SCL_pivot_5 = pivot_table.reset_index()\n",
    "\n",
    "# Added column new-column-70fj to dataset_SCL_pivot_5\n",
    "dataset_SCL_pivot_5.insert(3, 'new-column-70fj', 0)\n",
    "\n",
    "# Renamed new-column-70fj to Atraso por tipo de vuelo in dataset_SCL_pivot_5\n",
    "dataset_SCL_pivot_5.rename(columns={'new-column-70fj': 'Atraso por tipo de vuelo'}, inplace=True)\n",
    "\n",
    "# Set new-column-70fj in dataset_SCL_pivot_5 to =atraso_15 count 1/(atraso_15 count 0+atraso_15 count 1)\n",
    "dataset_SCL_pivot_5['Atraso por tipo de vuelo'] = dataset_SCL_pivot_5['atraso_15 count 1']/(dataset_SCL_pivot_5['atraso_15 count 0']+dataset_SCL_pivot_5['atraso_15 count 1'])\n",
    "\n",
    "# Renamed dataset_SCL_pivot_1 to Data atraso aerolinea\n",
    "Data_atraso_aerolinea = dataset_SCL_pivot_1\n",
    "\n",
    "# Renamed dataset_SCL_pivot to data atraso destino\n",
    "data_atraso_destino = dataset_SCL_pivot\n",
    "\n",
    "# Renamed dataset_SCL_pivot_2 to data atraso mes\n",
    "data_atraso_mes = dataset_SCL_pivot_2\n",
    "\n",
    "# Renamed dataset_SCL_pivot_3 to data atraso dia\n",
    "data_atraso_dia = dataset_SCL_pivot_3\n",
    "\n",
    "# Renamed dataset_SCL_pivot_4 to data atraso temporada\n",
    "data_atraso_temporada = dataset_SCL_pivot_4\n",
    "\n",
    "# Renamed dataset_SCL_pivot_5 to data atraso tipo vuelo\n",
    "data_atraso_tipo_vuelo = dataset_SCL_pivot_5\n",
    "\n",
    "# Pivoted dataset_SCL into df8\n",
    "unused_columns = dataset_SCL.columns.difference(set(['atraso_15']).union(set(['atraso_15'])).union(set({'atraso_15'})))\n",
    "tmp_df = dataset_SCL.drop(unused_columns, axis=1)\n",
    "pivot_table = tmp_df.pivot_table(\n",
    "    index=['atraso_15'],\n",
    "    columns=['atraso_15'],\n",
    "    values=['atraso_15'],\n",
    "    aggfunc={'atraso_15': ['count']}\n",
    ")\n",
    "pivot_table.set_axis([flatten_column_header(col) for col in pivot_table.keys()], axis=1, inplace=True)\n",
    "dataset_SCL_pivot = pivot_table.reset_index()\n",
    "\n",
    "# Added column new-column-w6rf to dataset_SCL_pivot\n",
    "dataset_SCL_pivot.insert(3, 'new-column-w6rf', 0)\n",
    "\n",
    "# Set new-column-w6rf in dataset_SCL_pivot to =atraso_15 count 1/(atraso_15 count 0+atraso_15 count 1)\n",
    "dataset_SCL_pivot['new-column-w6rf'] = dataset_SCL_pivot['atraso_15 count 1']/(dataset_SCL_pivot['atraso_15 count 0']+dataset_SCL_pivot['atraso_15 count 1'])\n",
    "\n",
    "# Set column atraso_15 count 0 at index 1 in dataset_SCL_pivot to 0\n",
    "dataset_SCL_pivot.at[1, 'atraso_15 count 0'] = 0\n",
    "dataset_SCL_pivot['new-column-w6rf'] = dataset_SCL_pivot['atraso_15 count 1']/(dataset_SCL_pivot['atraso_15 count 0']+dataset_SCL_pivot['atraso_15 count 1'])\n",
    "\n",
    "# Set column atraso_15 count 1 at index 0 in dataset_SCL_pivot to 0\n",
    "dataset_SCL_pivot.at[0, 'atraso_15 count 1'] = 0\n",
    "dataset_SCL_pivot['new-column-w6rf'] = dataset_SCL_pivot['atraso_15 count 1']/(dataset_SCL_pivot['atraso_15 count 0']+dataset_SCL_pivot['atraso_15 count 1'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que nos indican las tasas de atraso es que, partiendo por la tasa de atraso por destino, que el destino con mayor cantidad de atrasos totales es el aeropuesto Jorge Chavez de Perú, con 1214 atrasos y una tasa de atraso del 23%, seguido por el aeropuerto de Sao Paulo con un total de retrasos de 881 y una tasa de 24%. Nos guiamos primero por los destinos con mayor cantidad de retrasos totales ya que existen casos de retraso por destino del 100% pero corresponden a destinos con muestra de 1. \n",
    "\n",
    "La tasa de atraso correspondiente a la aerolínea nos indica que la aerolínea con mayor tasa de retraso es Plus Ultra Lineas Aereas con un 61%, seguida por Qantas Airways con un 57% y Air Canada con un 45%. Cabe nota que aunque grupo LATAM tiene la mayor cantidad de vuelos y atrasos totales, su tasa de retraso es solo del 18%.\n",
    "\n",
    "Las tasas de atraso por mes y día de la semana indican que los meses con mayor tasa de retraso son Julio y Diciembre, con un 29% y 25%, y que los días de la semana con mayor tasa de retraso son los días Viernes y Lunes con un 22% y 20% respectivamente.\n",
    "\n",
    "La tasa de atraso correspondiente a la temporada indica que en temporada alta hay un leve incremento en los atrasos, llegando a una tasa del 19% comparado con un 18% que corresponde a temporada baja. \n",
    "\n",
    "Por último, la tasa de atraso por tipo de vuelo indica que los vuelos internacionales tienen una mayor tasa de atraso con un 23%, comparado con los vuelos nacionales que tienen solo un 15%.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A su vez, y como se indicó en la respuesta a la pregunta 1, las variables que uno esperaría tuvieran mayor importancia en cuanto a la predicción de retrasos son la aerolínea y el tipo de vuelo, dadas las características mencionadas acerca de estas variables. En específico, su relevancia al cristalizar dimensiones institucionales (aerolineas) como también de magnitud de coordinación (tipo de vuelo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Entrena uno o varios modelos (usando el/los algoritmo(s) que prefieras) para estimar la probabilidad de atraso de un vuelo. Siéntete libre de generar variables adicionales y/o complementar con variables externas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la presente sección de elección de modelos, hemos hecho una revisión de la literatura respecto a la predicción de atrasos utilizando modelos de machine learning. Dada la cantidad de observaciones hemos optado por no tomar en cuenta modelos de deep learning ya que la data para entrenar el modelo sería insuficiente. Independiente de esta limitación, cabe notar que los resultados de Cheevachaipimol et al. (2021) indican que para la predicción de atrasos en vuelos, el modelo XGBOOST supera en rendimiento a modelos de redes neuronales tales como ANNs. \n",
    "\n",
    "La variable dependiente de nuestro modelo será atraso_15, ya que nuestro objetivo es predecir atrasos, y las variables independientes serán DIA, MES, DIANOM, TIPOVUELO, OPERA, SIGLADES, temporada_alta y periodo_dia. El resto de las variables presentes en el dataset son omitidas ya que redundan en términos de información con respecto a las variables seleccionadas. Las variables categóricas serán transformadas a dummy para que puedan ser pasadas a los algorítmos correspondientes. \n",
    "\n",
    "Ahora, teniendo en consideración las investigaciones de Cheevachaipimol et al. (2021), Kalyani et al (2020), y Shi & Xu (2021), hemos optado por centrarnos en el modelo de clásificación XGBOOST, dado que en todas las investigaciones mencionadas es el modelo con mejor rendimiento en cuánto a la predicción de atrasos en vuelos y viajes en tren. Se utilizará su versión destinada a clasificación dado que nuestra variable dependiente es binaria.\n",
    "\n",
    "Para tener un segundo modelo de comparación, hemos optado por un modelo de árbol de decisión, ya que en la investigación llevada a cabo por Yuemin Tang (2021) sobre predicción de atrasos de vuelos para el caso de EEUU. Cabe destacar, eso sí, que en dicha investigación no se utilizó el modelo XGBOOST, por lo que de manera explorativa mediremos el rendimiento de estos modelos respaldados por la literatura académica en relación a nuestro objetivo.\n",
    "\n",
    "Una última cuestión a considerar es también que nuestra variable a predecir (atraso_15) tiene un leve desbalance, ya que cerca del 82% de los datos indican que no hay atraso, en comparación al 18% restante que indica que sí hubo atraso en el vuelo. Es por ello que posterior a la comparación entre el modelo XGBOOST y de árbol de decisión se hará una optimización de los hiperparámetros para encontrar una mejora en términos de predicción utilizando técnicas para tratar datasets desbalanceados. Específicamente, XGBOOST cuenta con un hiperparámetro para lidar con este tipo de problemas que es scale_pos_weight, el cual cambiaremos en el caso de que salga como el mejor modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fecha_I',\n",
       " 'Vlo-I',\n",
       " 'Ori-I',\n",
       " 'Des-I',\n",
       " 'Emp-I',\n",
       " 'Fecha_O',\n",
       " 'Vlo-O',\n",
       " 'Ori-O',\n",
       " 'Des-O',\n",
       " 'Emp-O',\n",
       " 'DIA',\n",
       " 'MES',\n",
       " 'AÑO',\n",
       " 'DIANOM',\n",
       " 'TIPOVUELO',\n",
       " 'OPERA',\n",
       " 'SIGLAORI',\n",
       " 'SIGLADES',\n",
       " 'dif_min',\n",
       " 'atraso_15',\n",
       " 'temporada_alta',\n",
       " 'periodo_dia']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "\n",
    "list(dataset_SCL.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "\n",
    "cleanup_nums = {\"DIANOM\":     {\"Lunes\": 1, \"Martes\": 2, \"Miercoles\": 3,\"Jueves\":4,\n",
    "                               \"Viernes\":5,\"Sabado\":6,\"Domingo\":7}}\n",
    "\n",
    "dataset_SCL = dataset_SCL.replace(cleanup_nums)\n",
    "dataset_SCL.head()\n",
    "\n",
    "dummies = pd.get_dummies(dataset_SCL, columns=[\"TIPOVUELO\", \"SIGLADES\",\"OPERA\",\"periodo_dia\"])\n",
    "\n",
    "dataset_SCL = pd.concat([dataset_SCL,dummies],axis=\"columns\")\n",
    "\n",
    "dataset_SCL = dataset_SCL.drop([\"TIPOVUELO\",\"SIGLADES\",\"OPERA\",\"periodo_dia\"], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mitosheet import *; register_analysis('UUID-e84d4044-bc5a-45b0-a093-82d25319afd4')\n",
    "    \n",
    "# Imported dataset.csv\n",
    "import pandas as pd\n",
    "dataset = dataset_SCL\n",
    "\n",
    "dataset.to_csv(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187cf9d7165141ff8406e5d764603656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MitoWidget(analysis_data_json='{\"analysisName\": \"UUID-041c9868-06fd-4343-8bfc-e10f0dfc649b\", \"code\": {\"imports…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mitosheet.sheet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mitosheet import *; register_analysis('UUID-041c9868-06fd-4343-8bfc-e10f0dfc649b')\n",
    "    \n",
    "# Imported dataset.csv\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv(r'dataset.csv')\n",
    "\n",
    "# Deleted column Fecha_I from dataset\n",
    "dataset.drop(['Fecha_I'], axis=1, inplace=True)\n",
    "\n",
    "# Deleted column Vlo-I from dataset\n",
    "dataset.drop(['Vlo-I'], axis=1, inplace=True)\n",
    "\n",
    "# Deleted column Ori-I from dataset\n",
    "dataset.drop(['Ori-I'], axis=1, inplace=True)\n",
    "\n",
    "# Deleted column Des-I from dataset\n",
    "dataset.drop(['Des-I'], axis=1, inplace=True)\n",
    "\n",
    "# Deleted column Emp-I from dataset\n",
    "dataset.drop(['Emp-I'], axis=1, inplace=True)\n",
    "\n",
    "# Deleted column Fecha_O from dataset\n",
    "dataset.drop(['Fecha_O'], axis=1, inplace=True)\n",
    "\n",
    "# Deleted column Vlo-O from dataset\n",
    "dataset.drop(['Vlo-O'], axis=1, inplace=True)\n",
    "\n",
    "# Deleted column Ori-O from dataset\n",
    "dataset.drop(['Ori-O'], axis=1, inplace=True)\n",
    "\n",
    "# Deleted column Des-O from dataset\n",
    "dataset.drop(['Des-O'], axis=1, inplace=True)\n",
    "\n",
    "# Deleted column Emp-O from dataset\n",
    "dataset.drop(['Emp-O'], axis=1, inplace=True)\n",
    "\n",
    "# Deleted column AÑO from dataset\n",
    "dataset.drop(['AÑO'], axis=1, inplace=True)\n",
    "\n",
    "# Deleted column SIGLAORI from dataset\n",
    "dataset.drop(['SIGLAORI'], axis=1, inplace=True)\n",
    "\n",
    "# Deleted column dif_min from dataset\n",
    "dataset.drop(['dif_min'], axis=1, inplace=True)\n",
    "\n",
    "# Deleted column atraso_15 from dataset\n",
    "dataset.drop(['atraso_15'], axis=1, inplace=True)\n",
    "\n",
    "# Deleted column Fecha_I.1 from dataset\n",
    "dataset.drop(['Fecha_I.1'], axis=1, inplace=True)\n",
    "\n",
    "# Deleted column Vlo-I.1 from dataset\n",
    "dataset.drop(['Vlo-I.1'], axis=1, inplace=True)\n",
    "\n",
    "# Deleted column Ori-I.1 from dataset\n",
    "dataset.drop(['Ori-I.1'], axis=1, inplace=True)\n",
    "\n",
    "# Deleted column Des-I.1 from dataset\n",
    "dataset.drop(['Des-I.1'], axis=1, inplace=True)\n",
    "\n",
    "# Deleted column Emp-I.1 from dataset\n",
    "dataset.drop(['Emp-I.1'], axis=1, inplace=True)\n",
    "\n",
    "# Deleted column Fecha_O.1 from dataset\n",
    "dataset.drop(['Fecha_O.1'], axis=1, inplace=True)\n",
    "\n",
    "# Deleted column Vlo-O.1 from dataset\n",
    "dataset.drop(['Vlo-O.1'], axis=1, inplace=True)\n",
    "\n",
    "# Deleted column Ori-O.1 from dataset\n",
    "dataset.drop(['Ori-O.1'], axis=1, inplace=True)\n",
    "\n",
    "# Deleted column Des-O.1 from dataset\n",
    "dataset.drop(['Des-O.1'], axis=1, inplace=True)\n",
    "\n",
    "# Deleted column Emp-O.1 from dataset\n",
    "dataset.drop(['Emp-O.1'], axis=1, inplace=True)\n",
    "\n",
    "# Deleted column DIA.1 from dataset\n",
    "dataset.drop(['DIA.1'], axis=1, inplace=True)\n",
    "\n",
    "# Deleted column MES.1 from dataset\n",
    "dataset.drop(['MES.1'], axis=1, inplace=True)\n",
    "\n",
    "# Deleted column AÑO.1 from dataset\n",
    "dataset.drop(['AÑO.1'], axis=1, inplace=True)\n",
    "\n",
    "# Deleted column DIANOM.1 from dataset\n",
    "dataset.drop(['DIANOM.1'], axis=1, inplace=True)\n",
    "\n",
    "# Deleted column SIGLAORI.1 from dataset\n",
    "dataset.drop(['SIGLAORI.1'], axis=1, inplace=True)\n",
    "\n",
    "# Deleted column dif_min.1 from dataset\n",
    "dataset.drop(['dif_min.1'], axis=1, inplace=True)\n",
    "\n",
    "# Deleted column atraso_15.1 from dataset\n",
    "dataset.drop(['atraso_15.1'], axis=1, inplace=True)\n",
    "\n",
    "# Deleted column temporada_alta.1 from dataset\n",
    "dataset.drop(['temporada_alta.1'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_SCL_Features = dataset[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split de data\n",
    "\n",
    "X = dataset_SCL_Features # Features\n",
    "y = dataset_SCL.atraso_15 # Target variable\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '12:00 a 18:59'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m clf \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Train Decision Tree Classifer\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m clf \u001b[38;5;241m=\u001b[39m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#Predict the response for test dataset\u001b[39;00m\n\u001b[0;32m      8\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\tree\\_classes.py:937\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[0;32m    900\u001b[0m     \u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, X_idx_sorted\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    901\u001b[0m ):\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \n\u001b[0;32m    904\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 937\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_idx_sorted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_idx_sorted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    944\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\tree\\_classes.py:165\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    163\u001b[0m check_X_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(dtype\u001b[38;5;241m=\u001b[39mDTYPE, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    164\u001b[0m check_y_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 165\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_separately\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcheck_X_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_y_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n\u001b[0;32m    169\u001b[0m     X\u001b[38;5;241m.\u001b[39msort_indices()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\base.py:578\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate_separately:\n\u001b[0;32m    573\u001b[0m     \u001b[38;5;66;03m# We need this because some estimators validate X and y\u001b[39;00m\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;66;03m# separately, and in general, separately calling check_array()\u001b[39;00m\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;66;03m# on X and y isn't equivalent to just calling check_X_y()\u001b[39;00m\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;66;03m# :(\u001b[39;00m\n\u001b[0;32m    577\u001b[0m     check_X_params, check_y_params \u001b[38;5;241m=\u001b[39m validate_separately\n\u001b[1;32m--> 578\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_X_params)\n\u001b[0;32m    579\u001b[0m     y \u001b[38;5;241m=\u001b[39m check_array(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\utils\\validation.py:746\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    744\u001b[0m         array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mastype(dtype, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    745\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 746\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    749\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    750\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DataScience\\lib\\site-packages\\pandas\\core\\generic.py:2072\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2071\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype: npt\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m-> 2072\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '12:00 a 18:59'"
     ]
    }
   ],
   "source": [
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train,y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evalúa tu modelo. ¿Qué performance tiene? ¿Qué métricas usaste para evaluar esa performance y por qué? ¿Por qué elegiste ese algoritmo en particular? ¿Qué variables son las que más influyen en la predicción? ¿Cómo podrías mejorar la performance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referencias\n",
    "\n",
    "- \n",
    "\n",
    "- \n",
    "\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a3c468cb30380c08792cc00937d71a323e49ca99a71296cd0919105bf9ef30b4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
